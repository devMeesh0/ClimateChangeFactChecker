# -*- coding: utf-8 -*-
"""Climate Change Denial

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B7faQhElYUQLkmhPE8cFi45vjwB6WZZz
"""

import tensorflow as tf
import pandas as pd
from google.colab import files
from sklearn.model_selection import train_test_split
import io
import matplotlib.pyplot as plt
import numpy as np

uploaded = files.upload()

import json

tokenizer_json = tokenizer.to_json()
with io.open('tokenizer.json', 'w', encoding='utf-8') as f:
    f.write(json.dumps(tokenizer_json, ensure_ascii=False))

files.download("tokenizer.json")

cols = ("tweet", "existence", "existence.confidence")
df = pd.read_csv(io.BytesIO(uploaded['twitterglobalwarmingsentimentdata.csv']), encoding='unicode-escape')
df[cols[1]].replace({"Y":1, "Yes":1, "N":0, "No":0}, inplace=True)
df = df[df[cols[1]].notnull()]

num_words = 50000
max_len = 70

tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, oov_token="[OOV]")

train, test = train_test_split(df, test_size=0.1, shuffle=True, random_state=420)
tokenizer.fit_on_texts(train[cols[0]])

train_data = tokenizer.texts_to_sequences(train[cols[0]])
train_data = tf.keras.preprocessing.sequence.pad_sequences(train_data, maxlen=max_len)
train_labels = train[cols[1]]

test_data = tokenizer.texts_to_sequences(test[cols[0]])
test_data = tf.keras.preprocessing.sequence.pad_sequences(test_data, maxlen=max_len)
test_labels = test[cols[1]]

model = tf.keras.Sequential([
      tf.keras.layers.Embedding(num_words, 250, input_length=max_len),
      tf.keras.layers.GlobalAveragePooling1D(),
      tf.keras.layers.Dense(128, activation='relu'),
      tf.keras.layers.Dropout(0.3),
      tf.keras.layers.Dense(64),
      tf.keras.layers.Dropout(0.1),
      tf.keras.layers.Dense(16, activation='relu'),
      tf.keras.layers.Dense(1, activation="sigmoid")
])

model.summary()

loss_fn = tf.keras.losses.BinaryCrossentropy()
optimizer_fn = tf.keras.optimizers.Adamax(learning_rate=0.002)
model.compile(optimizer=optimizer_fn, loss=loss_fn)

epochs = 200
batch_size = 25

history = model.fit(train_data, train_labels, verbose=2, batch_size=batch_size, epochs=epochs)

plt.plot(history.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend('train', loc='upper left')
plt.show()

model.evaluate(test_data, test_labels, verbose=2)

answers = model.predict(test_data, verbose=2)

happyCounter = 0

for i in range(len(test_labels)):

  if abs( np.round(np.array(test_labels)[i]) - (np.round(answers[i])) ) == 0:
    happyCounter += 1

print (happyCounter)
print (len(test_labels))
print (happyCounter / len(test_labels))

stringsAsToken = tokenizer.texts_to_sequences(
    [
     "Climate change is real, climate change exists, we need to defeat climate change and promote environmentalism", 
     "Climate change is a hoax, global warming does not exist!",
     "George Soros uses China Climate Change (FAKE!) to control US DEEP STATE!!!!",
     "Climate change deniers are destroying this country! #solveclimatechange",
     "I love the environment! #climatechange #environment #stopclimatechange #solveclimatechange",
     "climate change is FAKE!!",
     "h"
     ])

model.predict(tf.ragged.constant(stringsAsToken))

from google.colab import files

export_dir='./ccdenial'

tf.saved_model.save(model, export_dir=export_dir)

!zip -r ./ccdenial.zip ./ccdenial

files.download("./ccdenial.zip")